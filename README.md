# Design Publications Scraper

Um sistema de web scraping para coletar publicaÃ§Ãµes acadÃªmicas relacionadas ao design de diversos repositÃ³rios digitais.

## ğŸ“ Estrutura do Projeto

```
design-publications-scraper/
â”œâ”€â”€ ğŸ“ scrapers/           # MÃ³dulos de scraping especÃ­ficos
â”‚   â”œâ”€â”€ base_scraper.py    # Classe base para todos os scrapers
â”‚   â”œâ”€â”€ arcosdesign_scraper.py
â”‚   â”œâ”€â”€ designetecnologia_scraper.py
â”‚   â”œâ”€â”€ educacaografica_scraper.py
â”‚   â”œâ”€â”€ estudosemdesign_scraper.py
â”‚   â”œâ”€â”€ humanfactorsindesign_scraper.py
â”‚   â”œâ”€â”€ infodesign_scraper.py

â”‚   â”œâ”€â”€ triades_scraper.py
â”‚   â””â”€â”€ template_scraper.py
â”œâ”€â”€ ğŸ“ utils/              # UtilitÃ¡rios e helpers
â”‚   â”œâ”€â”€ scrapers_factory.py
â”‚   â”œâ”€â”€ export_csv.py
â”‚   â”œâ”€â”€ html_parsing.py
â”‚   â””â”€â”€ deduplication.py   # Sistema de deduplicaÃ§Ã£o
â”œâ”€â”€ ğŸ“ configs/            # Arquivos de configuraÃ§Ã£o
â”‚   â””â”€â”€ config.yaml        # ConfiguraÃ§Ãµes principais
â”œâ”€â”€ ğŸ“ data/               # Dados coletados e processados
â”‚   â”œâ”€â”€ raw/               # Dados brutos (CSV, arquivos de saÃ­da)
â”‚   â””â”€â”€ processed/         # Dados processados e limpos
â”œâ”€â”€ ğŸ“ logs/               # Logs de execuÃ§Ã£o
â”œâ”€â”€ ğŸ“ tests/              # Testes unitÃ¡rios e de integraÃ§Ã£o
â”œâ”€â”€ ğŸ“ docs/               # DocumentaÃ§Ã£o adicional
â”œâ”€â”€ main.py                 # Ponto de entrada principal
â”œâ”€â”€ pipeline.py             # Pipeline de execuÃ§Ã£o dos scrapers
â”œâ”€â”€ deduplicate.py          # Script standalone de deduplicaÃ§Ã£o
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â””â”€â”€ README.md              # Este arquivo
```

## ğŸš€ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone <url-do-repositorio>
cd design-publications-scraper
```

2. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

## âš™ï¸ ConfiguraÃ§Ã£o

Edite o arquivo `configs/config.yaml` para configurar:
- **terms**: Termos de busca
- **repos**: RepositÃ³rios a serem consultados
- **max_pages**: NÃºmero mÃ¡ximo de pÃ¡ginas por busca
- **csv_filename**: Nome do arquivo de saÃ­da

## ğŸ¯ Uso

### ExecuÃ§Ã£o via Pipeline
```bash
python pipeline.py
```

### ExecuÃ§Ã£o via Main
```bash
python main.py
```

### DeduplicaÃ§Ã£o
```bash
# DeduplicaÃ§Ã£o automÃ¡tica (via pipeline)
python pipeline.py

# DeduplicaÃ§Ã£o manual
python deduplicate.py

# Criar nova base de dados
python deduplicate.py --create-base
```

## ğŸ“Š RepositÃ³rios Suportados

- **estudos_em_design**: Estudos em Design
- **infodesign**: InfoDesign

- **human_factors_in_design**: Human Factors in Design
- **arcos_design**: Arcos Design
- **design_e_tecnologia**: Design e Tecnologia
- **triades**: Triades
- **educacao_grafica**: EducaÃ§Ã£o GrÃ¡fica

## ğŸ”§ Desenvolvimento

### Adicionando um Novo Scraper

1. Crie um novo arquivo em `scrapers/`
2. Herde de `base_scraper.py`
3. Implemente o mÃ©todo `search()`
4. Adicione o scraper ao `ScrapterFactory`
5. Configure no `config.yaml`

### Estrutura de um Scraper

```python
from scrapers.base_scraper import BaseScraper

class MeuScraper(BaseScraper):
    def __init__(self):
        super().__init__("https://exemplo.com")
    
    def search(self, term, max_pages=5):
        # Implementar lÃ³gica de busca
        pass
```

## ğŸ“ Logs e DeduplicaÃ§Ã£o

Os logs de execuÃ§Ã£o sÃ£o salvos na pasta `logs/` para facilitar o debug e monitoramento.

### Sistema de DeduplicaÃ§Ã£o
O projeto inclui um sistema inteligente de deduplicaÃ§Ã£o que:
- Compara novos resultados com uma base de dados existente
- Identifica apenas registros realmente novos
- Atualiza automaticamente a base de dados
- Gera relatÃ³rios de estatÃ­sticas
- Evita duplicatas baseado no campo `link` dos artigos

## ğŸ§ª Testes

Execute os testes na pasta `tests/`:
```bash
python -m pytest tests/
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT.