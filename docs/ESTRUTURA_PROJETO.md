# ğŸ—ï¸ Estrutura do Projeto Design Publications Scraper

## ğŸ“‹ **VisÃ£o Geral da OrganizaÃ§Ã£o**

Este documento detalha a estrutura limpa e organizada do projeto, seguindo as melhores prÃ¡ticas de organizaÃ§Ã£o de projetos Python.

## ğŸ¯ **PrincÃ­pio de OrganizaÃ§Ã£o**

O projeto foi estruturado seguindo o princÃ­pio de **separaÃ§Ã£o clara de responsabilidades**:

- ğŸ¤– **`cli/`**: AutomaÃ§Ã£o e execuÃ§Ã£o em lote
- ğŸŒ **`web/`**: Interface manual e exploraÃ§Ã£o
- ğŸ“š **`src/`**: Core do sistema e lÃ³gica de negÃ³cio
- ğŸ“ **`data/`**: Dados processados e resultados
- ğŸ“– **`docs/`**: DocumentaÃ§Ã£o e guias

## ğŸ“ **Estrutura de DiretÃ³rios**

```
design-publications-scraper/
â”œâ”€â”€ ğŸ“ cli/                          # ğŸ¤– AutomaÃ§Ã£o e CLI
â”‚   â”œâ”€â”€ README.md                     # DocumentaÃ§Ã£o da CLI
â”‚   â”œâ”€â”€ run.py                        # Pipeline principal
â”‚   â”œâ”€â”€ run_cli.py                    # Interface CLI interativa
â”‚   â””â”€â”€ test_filtering.py             # Teste do sistema
â”œâ”€â”€ ğŸ“ web/                           # ğŸŒ Interface Web
â”‚   â”œâ”€â”€ README.md                     # DocumentaÃ§Ã£o da Web
â”‚   â”œâ”€â”€ streamlit_app.py              # AplicaÃ§Ã£o Streamlit
â”‚   â””â”€â”€ run_streamlit.py              # Script de entrada
â”œâ”€â”€ ğŸ“ src/                           # ğŸ“š Core do Sistema
â”‚   â””â”€â”€ ğŸ“ design_scraper/            # Pacote principal
â”‚       â”œâ”€â”€ ğŸ“ core/                  # LÃ³gica principal
â”‚       â”‚   â”œâ”€â”€ automated_pipeline.py # Pipeline automatizado
â”‚       â”‚   â”œâ”€â”€ manual_search.py      # Busca manual
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ ğŸ“ config/                # ConfiguraÃ§Ãµes
â”‚       â”‚   â”œâ”€â”€ config.yaml           # ConfiguraÃ§Ã£o principal
â”‚       â”‚   â””â”€â”€ manual_search_config.yaml
â”‚       â”œâ”€â”€ ğŸ“ scrapers/              # MÃ³dulos de scraping
â”‚       â”‚   â”œâ”€â”€ base_scraper.py       # Classe base
â”‚       â”‚   â”œâ”€â”€ estudosemdesign_scraper.py
â”‚       â”‚   â”œâ”€â”€ infodesign_scraper.py
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ ğŸ“ utils/                 # UtilitÃ¡rios
â”‚       â”‚   â”œâ”€â”€ data_transformer.py   # Filtros e transformaÃ§Ã£o
â”‚       â”‚   â”œâ”€â”€ deduplication.py      # DeduplicaÃ§Ã£o
â”‚       â”‚   â”œâ”€â”€ scrapers_factory.py   # Factory de scrapers
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“ data/                          # ğŸ“Š Dados e Resultados
â”‚   â”œâ”€â”€ ğŸ“ raw/                       # Dados brutos
â”‚   â”‚   â”œâ”€â”€ base_database.csv         # Base principal
â”‚   â”‚   â””â”€â”€ search_results.csv        # Resultados de busca
â”‚   â””â”€â”€ ğŸ“ processed/                 # Dados processados
â”‚       â”œâ”€â”€ filtered_results.csv      # ApÃ³s filtros
â”‚       â””â”€â”€ new_records.csv           # Novos registros
â”œâ”€â”€ ğŸ“ docs/                          # ğŸ“– DocumentaÃ§Ã£o
â”‚   â”œâ”€â”€ README_NEW_STRUCTURE.md       # DocumentaÃ§Ã£o da estrutura
â”‚   â””â”€â”€ ESTRUTURA_PROJETO.md         # Este arquivo
â”œâ”€â”€ ğŸ“ tests/                         # ğŸ§ª Testes
â”œâ”€â”€ ğŸ“ logs/                          # ğŸ“ Logs de execuÃ§Ã£o
â”œâ”€â”€ ğŸ“ requirements.txt                # DependÃªncias Python
â”œâ”€â”€ ğŸ“ config.yaml                    # ConfiguraÃ§Ã£o principal
â””â”€â”€ ğŸ“ README.md                      # DocumentaÃ§Ã£o principal
```

## ğŸ”„ **Fluxo de Dados**

### **Pipeline Automatizado (CLI)**
```
config.yaml â†’ automated_pipeline.py â†’ scrapers â†’ data_transformer.py â†’ deduplication.py â†’ arquivos de saÃ­da
```

### **Interface Manual (Web)**
```
streamlit_app.py â†’ manual_search.py â†’ scrapers â†’ filtros â†’ deduplicaÃ§Ã£o â†’ interface de resultados
```

## ğŸ¯ **SeparaÃ§Ã£o de Responsabilidades**

### **ğŸ¤– DiretÃ³rio `cli/` - AutomaÃ§Ã£o**
- **PropÃ³sito**: ExecuÃ§Ã£o automatizada e em lote
- **UsuÃ¡rios**: Sistemas, agendadores, produÃ§Ã£o
- **CaracterÃ­sticas**: 
  - Sem interface grÃ¡fica
  - Logs detalhados
  - ConfiguraÃ§Ã£o via arquivos
  - ExecuÃ§Ã£o programada

### **ğŸŒ DiretÃ³rio `web/` - Interface Manual**
- **PropÃ³sito**: Busca personalizada e exploraÃ§Ã£o
- **UsuÃ¡rios**: Pesquisadores, estudantes, profissionais
- **CaracterÃ­sticas**:
  - Interface grÃ¡fica intuitiva
  - ConfiguraÃ§Ã£o interativa
  - VisualizaÃ§Ã£o de resultados
  - Download de dados

### **ğŸ“š DiretÃ³rio `src/` - Core do Sistema**
- **PropÃ³sito**: LÃ³gica de negÃ³cio compartilhada
- **UsuÃ¡rios**: Ambos os cenÃ¡rios (CLI e Web)
- **CaracterÃ­sticas**:
  - MÃ³dulos reutilizÃ¡veis
  - ConfiguraÃ§Ã£o centralizada
  - LÃ³gica de scraping
  - Processamento de dados

## ğŸ”§ **ConfiguraÃ§Ã£o e PersonalizaÃ§Ã£o**

### **ConfiguraÃ§Ã£o Principal (`config.yaml`)**
- **LocalizaÃ§Ã£o**: Raiz do projeto
- **Uso**: Pipeline automatizado
- **ConteÃºdo**: RepositÃ³rios, termos, arquivos de saÃ­da

### **ConfiguraÃ§Ã£o Manual (`manual_search_config.yaml`)**
- **LocalizaÃ§Ã£o**: `src/design_scraper/config/`
- **Uso**: Interface web
- **ConteÃºdo**: Limites, opÃ§Ãµes de interface, configuraÃ§Ãµes de filtros

## ğŸ“Š **GestÃ£o de Dados**

### **DiretÃ³rio `data/raw/`**
- **`base_database.csv`**: Base de dados principal (nÃ£o modificada automaticamente)
- **`search_results.csv`**: Resultados brutos dos scrapers

### **DiretÃ³rio `data/processed/`**
- **`filtered_results.csv`**: Resultados apÃ³s aplicaÃ§Ã£o de filtros
- **`new_records.csv`**: Novos registros Ãºnicos (para revisÃ£o manual)

## ğŸ§ª **Testes e ValidaÃ§Ã£o**

### **Teste do Sistema (`cli/test_filtering.py`)**
- Valida o funcionamento dos filtros
- Testa a transformaÃ§Ã£o de dados
- Verifica a deduplicaÃ§Ã£o
- Gera relatÃ³rio de funcionamento

### **Testes UnitÃ¡rios (`tests/`)**
- Testes individuais de mÃ³dulos
- ValidaÃ§Ã£o de funcionalidades especÃ­ficas
- Cobertura de cÃ³digo

## ğŸ“ **Logs e Monitoramento**

### **DiretÃ³rio `logs/`**
- Logs de execuÃ§Ã£o do pipeline
- HistÃ³rico de operaÃ§Ãµes
- Debugging e troubleshooting
- Monitoramento de performance

## ğŸš€ **Casos de Uso por DiretÃ³rio**

### **ğŸ¤– Use `cli/` quando:**
- Executar buscas em lote
- Agendar execuÃ§Ãµes automÃ¡ticas
- Integrar com outros sistemas
- ProduÃ§Ã£o e manutenÃ§Ã£o

### **ğŸŒ Use `web/` quando:**
- Fazer buscas personalizadas
- Explorar resultados interativamente
- Configurar parÃ¢metros especÃ­ficos
- Download de dados especÃ­ficos

### **ğŸ“š Use `src/` quando:**
- Desenvolver novas funcionalidades
- Modificar scrapers existentes
- Personalizar filtros e transformaÃ§Ãµes
- Estender o sistema

## ğŸ”„ **ManutenÃ§Ã£o e AtualizaÃ§Ãµes**

### **Adicionar Novo Scraper**
1. Crie arquivo em `src/design_scraper/scrapers/`
2. Herde de `base_scraper.py`
3. Implemente mÃ©todo `search()`
4. Adicione ao `scrapers_factory.py`
5. Configure em `config.yaml`

### **Modificar Filtros**
1. Edite `src/design_scraper/utils/data_transformer.py`
2. Modifique lista `required_keywords`
3. Ajuste funÃ§Ã£o `is_portuguese_title()`
4. Teste com `cli/test_filtering.py`

### **Personalizar Interface**
1. Edite `web/streamlit_app.py`
2. Modifique `repo_options`
3. Ajuste validaÃ§Ãµes e layout
4. Teste com `streamlit run web/streamlit_app.py`

## ğŸ“ **Suporte e Troubleshooting**

### **Problemas de ImportaÃ§Ã£o**
- Execute sempre da raiz do projeto
- Verifique se `src/` estÃ¡ no Python path
- Confirme estrutura de diretÃ³rios

### **Problemas de ConfiguraÃ§Ã£o**
- Verifique `config.yaml` na raiz
- Confirme caminhos dos arquivos
- Valide formato YAML

### **Problemas de ExecuÃ§Ã£o**
- Use `cli/test_filtering.py` para validaÃ§Ã£o
- Verifique logs em `logs/`
- Confirme dependÃªncias instaladas

---

**ğŸ¯ Estrutura limpa e organizada para mÃ¡xima clareza e manutenibilidade!**
